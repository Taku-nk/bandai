# 2020/4-5 バンダイナムコ研究所データサイエンスコンペティション(小町ちゃんのノイズ除去)で使用したコードの紹介。

#　ノイズ除去過程はJupyter Notebookにて行いましたが、公開できるほどには綺麗にまとまらなかったので、除去モデルの根幹になる関数のみをmodel.pyファイルに記載しています。




**ノイズ除去方法**

概要:
    Melスペクトログラムを画像に見立てて、2D畳み込みニューラルネットオートエンコーダー・デコーダーをつかってMelスペクトログラムを直接(波データを復元することなく)ノイズ除去した。
    与えられたノイズ付きデータnoised_tgtのデータのノイズは、Max Pooling 2D によって無視できるのではないかという仮定をして、いったん情報を4分の1に圧縮し、その圧縮データをもとに再度拡大することで、ノイズの除去を試みた。
    つまり情報を圧縮すれば、ノイズのような、データに含まれる小さな乱れは無視され、データの本質的な部分のみが圧縮データに残され、それを再度拡大したらノイズは消えているのではないか、ということを考えた。
    またノイズなしのrawデータのみでトレーニングさせた、rawデータの綺麗な形を復元するデコーダーを使うことで、デコーダーはnoise付きデータも、拡大後にrawデータに似た綺麗な形に復元できるのではないかと考えた。
    
    結果、与えられたrawデータ100個を復元できるようにトレーニングさせたオートエンコーダー・デコーダーでも、ノイズなしのデータには程遠いが、耳で聞いて、多少のノイズを除去できていることがわかった。またスペクトログラムを目で見た限りではノイズが消えスムーズになったことが確認できた。
　


詳細:

トレーニングデータセット
    与えられた100個のrawデータのみ。



データの前処理
　
    与えられたデータの値は、最小値が0、最大値が420くらいで、0付近の値が特に多いので、16乗根をもとにした関数を使い、特に0付近での値の変化に敏感に反応するように変換した。(データの値をdBにする方法も考えたが、最大値420などの大きな値の領域における変化に鈍感すぎるのでつかわなかった。)
    さらにニューラルネットをトレーニングするために、変換したデータが0から1の範囲に収まるように割り算をして正規化した。



畳み込みニューラルネットオートエンコーダー・デコーダーの概要
    まず初めに、pooling層で情報を圧縮して、その圧縮データをもとのサイズに復元するという形で、トレーニングさせた。
    発想は超解像技術である。
    ※ニューラルネットはTensorflowを使用して作った。


トレーニング時
    input : 与えられたrawデータ
    target: input

以下オートエンコーダー・デコーダーの構造
    Max Pooling2D　# 情報を4分の1にする。　つまり大きさは半分
    
    Conv2D(channels=8, filter_size=(5, 5))
    Batch Normalization
    ReLU 

    Conv2D(channels=16, filter_size=(5, 5))
    Batch Normalization
    ReLU     
    
    Conv2D(channels=32, filter_size=(5, 5))
    Batch Normalization
    ReLU 
    
    Upsampling2D　　　# 大きさを二倍にする。つまり情報を4倍にする
    
    Conv2D(channels=16, filter_size=(5, 5))
    Batch Normalization
    ReLU 

    Conv2D(channels=8, filter_size=(5, 5))
    Batch Normalization
    ReLU 

    Conv2D(channels=1, filter_size(3, 3))
    Sigmoid

output: inputと同じ形の行列

損失関数: Mean Squared Error

以上、オートエンコーダー・デコーダーの構造


オートエンコーダー・デコーダーの説明
    まず最初に4分の1に圧縮する(Max Pooling 2D)。
    それぞれの畳み込み層ではinputデータとの形が変わらないようにパディングをした。また、フィルターのサイズは(2, 2)や(3, 3)、(7, 7)も試したが、(5, 5)のテストスコアがもっともよかった。
    Batch Normalization を使うことでそれぞれのニューロンがなるべく均等に作用するようにし、重み変数の発散や0になって使い物にならなくなるなどの現象を起こりにくいようにした。
    トレーニング可能な変数の数は約3万個。
  
 